# -*- coding: utf-8 -*-
"""CSE 4108 Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uXqx27kJ1AWZKJEdCGwQnnGp7v1sSHnc

## **Importing Library**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import BayesianRidge
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_percentage_error
from sklearn import metrics
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

cd'/content/drive/MyDrive/Colab Notebooks'

"""## **Importing Dataset**"""

data = pd.read_excel("used_car_data.xlsx")

data.head()

data.columns

data.describe()

data.info()

"""# **Data Visualization**

## Correlations with Price
"""

sns.jointplot(x='Milleage',y='Price',data=data,kind='reg')

sns.jointplot(x='Price',y='horsepower',data=data,kind='reg')

sns.jointplot(x='Price',y='year',data=data,kind='reg')

sns.set(rc={'figure.figsize':(15.7,18.27)})
sns.barplot(x='Exterior Color', y = 'Price', data=data)

sns.set(rc={'figure.figsize':(8,8)})
sns.distplot(data['Price'])

"""## Toyota Vs Honda"""

df_toyota = data[data['Brand'] == 'Toyota']

df_honda = data[data['Brand'] == 'Honda']

sns.set(rc={'figure.figsize':(8.7,12.27)})
sns.lineplot(data=data, x="year", y="Price", hue='Brand', style='Brand')

sns.set(rc={'figure.figsize':(8.7,12.27)})
sns.lineplot(data=data, x="year", y="Milleage", style='Brand')

sns.lmplot(y='Price', x='Milleage', data=data, hue='Brand',markers=['o','v'])

sns.lmplot(y='Price', x='year', data=data, hue='Brand', palette='magma')

sns.boxplot(x='year', y='Price', data=data, hue='Brand')

"""# Data Preprocesssing"""

data1 = data.copy()

data1['Drive Type'].value_counts()

data1['Engine Type'].value_counts()

"""## Generating new features from existing ones"""

data1[['No. of Axle','Axle Type']] = data1['Drive Type'].str.split(',', 1, expand=True)
data1[['Cylinder','Engine Type2']] = data1['Engine Type'].str.split(',', 1, expand=True)
data1[['Gas Type','Litres']] = data1['Engine Type2'].str.split(',', 1, expand=True)
data1[['MPG1','MPG2']] = data1['MPG'].str.split('/', 1, expand=True)
data1['MPG1'] = data1['MPG1'].str.replace("\scity", "")
data1['MPG2'] = data1['MPG2'].str.replace("\shwy", "")
data1.head()

data2 = data1.copy()

data2.columns

data2.drop(['Engine Type','Engine Type2','Drive Type','MPG'],axis=1, inplace=True)

"""## Mapping"""

data2.columns

data2['year'].value_counts()

data2['year'] = data2['year'] - 2000

data2['year'].value_counts()

data2['Litres'].value_counts()

data2['Litres'] = data2['Litres'].map(lambda x: x.rstrip('L'))
data2['Litres'].value_counts()

data2['Gas Type'].value_counts()

def impute_gas_type(cols):
    if cols == ' Hybrid':
        return 2
    elif cols == ' Turbo Gas':
        return 1
    elif cols == ' Gas':
        return 0

data2['Gas Type'] = data2['Gas Type'].apply(impute_gas_type)

data2['Gas Type'].value_counts()

data2['Cylinder'].value_counts()

data2['Cylinder'] = data2['Cylinder'].map(lambda x: x.rstrip('-cyl'))
data2['Cylinder'].value_counts()

data2['No. of Axle'].value_counts()

data2['No. of Axle'] = data2['No. of Axle'].map(lambda x: x.rstrip('/AWD'))
data2['No. of Axle'].value_counts()

data2['Axle Type'].value_counts()

def impute_drive_type_2(cols):    #front wheel drive are least expensive, FWD is most expensive
    if cols == ' Front Wheel Drive':
        return 0
    elif cols == ' Rear Wheel Drive':
        return 1
    elif cols == ' All Wheel':
        return 2
    elif cols == ' All Wheel Drive':
        return 2
    elif cols == ' Four Wheel Drive':
        return 3
    elif cols == ' 4-Wheel Drive':
        return 3
    else: 
        return 0

data2['Axle Type'] = data2['Axle Type'].apply(impute_drive_type_2)
data2['Axle Type'].value_counts()

data2['Brand'].value_counts()

data2['Exterior Color'].value_counts()

def impute_color(cols):
    if cols == 'White':
        return 7
    elif cols == 'Gray':
        return 4
    elif cols == 'Black':
        return 5
    elif cols == 'Silver' or cols == 'silver':
        return 3
    elif cols == 'Blue':
        return 3
    elif cols == 'Red':
        return 4
    elif cols == 'Burgundy' or cols =='Burgyndy':
        return 5
    elif cols == 'Brown':
        return 5
    elif cols == 'Tan':
        return 6
    elif cols == 'Orange':
        return 7
    elif cols == 'Gold':
        return 2
    elif cols == 'Maroon':
        return 8
    elif cols == 'Purple':
        return 1
    elif cols == 'Green':
        return 2
    elif cols == 'Pearl':
        return 2
    else: 
        return 1

data2['Exterior Color'] = data2['Exterior Color'].apply(impute_color)
data2['Exterior Color'].value_counts()

data3 = data2.copy()

"""## Converting to numeric Dtype from object Dtype"""

data3.info()

data3['MPG1'] = pd.to_numeric(data3['MPG1'])
data3['MPG2'] = pd.to_numeric(data3['MPG2'])
data3['Litres'] = pd.to_numeric(data3['Litres'])
data3['Cylinder'] = pd.to_numeric(data3['Cylinder'])
data3['No. of Axle'] = pd.to_numeric(data3['No. of Axle'])
data3['Gas Type'] = pd.to_numeric(data3['Gas Type'])
data3['Exterior Color'] = pd.to_numeric(data3['Exterior Color'])

data3.info()

"""## Get Dummies"""

data4 = data3.copy()

data4 = pd.get_dummies(data4, columns=['Brand'])

data4.head(10)

data4.info()

"""## Processing model features"""

data5 = data4.copy()

data5['Model'].value_counts()

data5[['Model_Type1','Model_Type2']] = data5['Model'].str.split(' ', 1, expand=True)

data5['Model_Type1'].value_counts()

data5['Model_Type2'].value_counts()

data5[['Model_Type3','Model_Type4']] = data5['Model_Type2'].str.split(' ', 1, expand=True)

data5['Model_Type3'].value_counts()

data5['Model_Type4'].value_counts()

data5.drop(['Model','Model_Type2'],axis=1, inplace=True)

data5.head(10)

data6 = data5.copy()

data6 = pd.get_dummies(data6, columns=['Model_Type1'])

data6 = pd.get_dummies(data6, columns=['Model_Type3'])

data6 = pd.get_dummies(data6, columns=['Model_Type4'])

data6.columns[33:111] # all models

data6.head()

data7 = data6.copy()

data7.info()

data7.to_excel('exported.xlsx')

"""## Checking if any null values generated during preprocessing"""

np.any(np.isnan(data7))

nan_rows = data7[data7.isnull().T.any()]

nan_rows.head(10)

data8 = data7.copy()

data8.head()

data8.columns

"""## Standard Scaler"""

#Standard Scaler
data8[['Milleage','horsepower', 'year', 'Passenger Capacity', 'MPG1', 'MPG2', 'No. of Axle', 'Litres', 'Cylinder', 'Exterior Color', 'Axle Type', 'Gas Type']] = StandardScaler().fit_transform(data8[['Milleage','horsepower', 'year', 'Passenger Capacity', 'MPG1', 'MPG2', 'No. of Axle', 'Litres', 'Cylinder', 'Exterior Color', 'Axle Type', 'Gas Type']])

data8.head(10)

#MinMax Scaler
#data8[['Milleage','horsepower', 'year', 'Passenger Capacity', 'MPG1', 'MPG2', 'No. of Axle', 'Litres', 'Cylinder']] = MinMaxScaler().fit_transform(data8[['Milleage','horsepower', 'year', 'Passenger Capacity', 'MPG1', 'MPG2', 'No. of Axle', 'Litres', 'Cylinder']])

mae_lst = []
mse_lst = []
rmse_lst = []
r2_lst = []
cross_val = []

X = data8.drop(['Price'],axis=1).values
y = data8['Price'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

"""#Evaluation Metric

---

Description Here
"""

def evaluation_abs_error(y_test, predictions):
    return metrics.mean_absolute_error(y_test, predictions)

def evaluation_mean_squared_error(y_test, predictions):
    return metrics.mean_squared_error(y_test, predictions)

def evaluation_root_mean_squared_error(y_test, predictions):
    return np.sqrt(metrics.mean_squared_error(y_test, predictions))

def evaluation_r2_score_error(y_test, predictions):
    return metrics.r2_score(y_test, predictions)

def evaluate(y_test, predictions):    
    mae = evaluation_abs_error(y_test, predictions)            
    mse = evaluation_mean_squared_error(y_test, predictions)    
    rmse = evaluation_root_mean_squared_error(y_test, predictions)
    r2 = evaluation_r2_score_error(y_test, predictions)

    print('MAE: ', mae)
    print('MSE: ', mse)
    print('RMSE: ', rmse)
    print('R2: ', r2)
    
    mae_lst.append(mae)
    mse_lst.append(mse)
    rmse_lst.append(rmse)
    r2_lst.append(r2)

"""#Linear Regression (Our base model)

---

Linear Regression is the supervised Machine Learning model in which the model finds the best fit linear line between the independent and dependent variable i.e it finds the linear relationship between the dependent and independent variable.
"""

lm = LinearRegression()

lm.fit(X_train,y_train)

lm_predictions = lm.predict(X_test)

evaluate(y_test, lm_predictions)

val = np.average(cross_val_score(lm, X, y,cv=4))
cross_val.append(val)
print(val)

"""# RandomForest Regressor

---

This model has the highest accuracy as the prediction is made by averaging each of the predictions of the each tree of forest.


"""

random_forest = RandomForestRegressor(random_state=11)

random_forest.fit(X,y)

rf_predictions = random_forest.predict(X_test)

evaluate(y_test, rf_predictions)

val = np.average(cross_val_score(random_forest, X, y,cv=4))
cross_val.append(val)
print(val)

"""# Decision Tree

---
Poor Resolution on Data With Complex Relationships Among the Variables. 
example: complex relation between price and drive type.
"""

decision_tree = DecisionTreeRegressor(random_state=0)
decision_tree.fit(X_train, y_train)

dt_predictions = decision_tree.predict(X_test)

evaluate(y_test, dt_predictions)

val = np.average(cross_val_score(decision_tree, X, y,cv=4))
cross_val.append(val)
print(val)

"""# Bayesian Ridge

---

Bayesian ridge estimates a probabilistic model of the regression issue. A spherical Gaussian provides the prior for the coefficient: p(w|𝝀) = Ɲ(w|0,𝜆-1 Ip) The priors over and are gamma distributions, which are the conjugate prior for Gaussian precision. Bayesian Ridge Regression is the name given to the resulting model, which is comparable to the classic Ridge.It performs well in cases of large multivariate data.


"""

bayesian_ridge = BayesianRidge()
bayesian_ridge.fit(X_train, y_train)

br_predictions = bayesian_ridge.predict(X_test)

evaluate(y_test, br_predictions)

val = np.average(cross_val_score(bayesian_ridge, X, y,cv=4))
cross_val.append(val)
print(val)

"""# K Nearest Neighbours Regressor
For our dataset which included majority points from others that's why could not perform that much good.

"""

knn_r = KNeighborsRegressor(n_neighbors=3)
knn_r.fit(X_train, y_train)
knn_r_predictions = knn_r.predict(X_test)

evaluate(y_test, knn_r_predictions)

val = np.average(cross_val_score(knn_r, X, y,cv=4))
cross_val.append(val)
print(val)

"""# Voting Regressor"""

vreg = VotingRegressor([('Random Forest', random_forest), ('K Nearest Neighbour', knn_r),('Decision Tree',decision_tree),('Bayesian Ridge',bayesian_ridge)],weights=[0.60,0.20,0.15,0.05])

voting_pred = vreg.fit(X_train, y_train).predict(X_test)

evaluate(y_test, voting_pred)

val = np.average(cross_val_score(vreg, X, y,cv=4))
cross_val.append(val)
print(val)

"""# Performance Table"""

data = []
model_names = ['Linear Regression','Random Forest', 'Decision Tree', 'Bayesian Ridge', 'K Nearest Neighbour', 'Voting Regressor']
for i in range(0,6,1):    
    data.append([model_names[i],format(mae_lst[i], '.4f'),format(mse_lst[i], '.4f'),format(rmse_lst[i], '.4f'),  format(r2_lst[i], '.4f') , format(cross_val[i], '.4f')])

model_eval = pd.DataFrame(data, columns = ['Name', 'Mean Absolute Error', 'Mean Square Error', 'Root Mean Square Error', 'R2 Score', 'Cross Validation Score'])

model_eval